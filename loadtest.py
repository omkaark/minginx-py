# generated by GPT

import time
import requests
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

URLS = [
    "http://127.0.0.1:3000/index.html",
    "http://127.0.0.1:3000/script.js",
    "http://127.0.0.1:3000/style.css",
]

NUM_CLIENTS = 100          # concurrent clients
REQUESTS_PER_CLIENT = 50   # how many requests each client sends

results = []
failures = 0

def hit(url):
    latencies = []
    for _ in range(REQUESTS_PER_CLIENT):
        try:
            start = time.perf_counter()
            r = requests.get(url)
            duration = time.perf_counter() - start
            if r.status_code == 200:
                latencies.append(duration)
            else:
                print(f"Failed: {url} - Status {r.status_code}")
        except Exception as e:
            print(f"Exception for {url}: {e}")
            global failures
            failures += 1
    return latencies

start_time = time.perf_counter()

with ThreadPoolExecutor(max_workers=NUM_CLIENTS) as pool:
    futures = []
    for i in range(NUM_CLIENTS):
        url = URLS[i % len(URLS)]
        futures.append(pool.submit(hit, url))

    for future in as_completed(futures):
        results.extend(future.result())

total_time = time.perf_counter() - start_time
total_requests = NUM_CLIENTS * REQUESTS_PER_CLIENT

print("\n==== Load Test Summary ====")
print(f"Total Requests: {total_requests}")
print(f"Successful Responses: {len(results)}")
print(f"Failed Requests: {failures}")
print(f"Total Time: {total_time:.2f}s")
print(f"Throughput: {total_requests / total_time:.2f} req/s")

if results:
    print(f"Latency min: {min(results) * 1000:.2f} ms")
    print(f"Latency avg: {statistics.mean(results) * 1000:.2f} ms")
    print(f"Latency p95: {statistics.quantiles(results, n=100)[94] * 1000:.2f} ms")
    print(f"Latency max: {max(results) * 1000:.2f} ms")
